#!/usr/bin/env python3
"""Collect daily trend data for findings/correlations and emit a CSV for dashboards."""

import argparse
import csv
import json
import subprocess
import sys
from collections import Counter
from datetime import datetime, timezone
from pathlib import Path


def sis_version() -> str:
    try:
        proc = subprocess.run(["sis", "--version"], check=True, capture_output=True, text=True)
        return proc.stdout.strip()
    except subprocess.CalledProcessError:
        return "unknown"


def run_query_file(pdf_path: Path, query: str, output_format: str) -> list[dict]:
    cmd = [
        "sis",
        "query",
        str(pdf_path),
        query,
        "--format",
        output_format,
    ]
    proc = subprocess.run(cmd, check=True, capture_output=True, text=True)
    lines = [line for line in proc.stdout.splitlines() if line.strip()]
    return [json.loads(line) for line in lines]


def find_pdf_files(corpus: Path, glob: str) -> list[Path]:
    return sorted(p for p in corpus.rglob(glob) if p.is_file())


def is_pdf(path: Path) -> bool:
    try:
        with path.open("rb") as f:
            header = f.read(5)
            return header.startswith(b"%PDF-")
    except OSError:
        return False


def aggregate_findings(records: list[dict]) -> tuple[Counter, set[int]]:
    counter = Counter()
    files = set()
    for record in records:
        files.add(record.get("file"))
        for finding in record.get("result") or []:
            key = (
                finding.get("kind"),
                finding.get("severity"),
                finding.get("surface"),
                finding.get("confidence"),
            )
            counter[key] += 1
    return counter, files


def aggregate_correlations(records: list[dict]) -> Counter:
    counter = Counter()
    for record in records:
        for pattern, summary in (record.get("result") or {}).items():
            count = summary.get("count", 0)
            counter[(pattern, summary.get("severity", "Unknown"))] += count
    return counter


def write_rows(
    csv_path: Path,
    rows: list[dict],
    header: list[str],
) -> None:
    mode = "a" if csv_path.exists() else "w"
    with csv_path.open(mode, newline="") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=header)
        if mode == "w":
            writer.writeheader()
        writer.writerows(rows)


def build_rows(
    date_iso: str,
    sis_version_str: str,
    total_files: int,
    finding_counts: Counter,
    correlation_counts: Counter,
) -> list[dict]:
    rows: list[dict] = []
    for (kind, severity, surface, confidence), count in finding_counts.items():
        rows.append(
            {
                "date": date_iso,
                "sis_version": sis_version_str,
                "type": "finding",
                "kind_or_pattern": kind,
                "severity": severity,
                "surface": surface,
                "confidence": confidence,
                "count": count,
                "files_scanned": total_files,
                "notes": None,
            }
        )
    for (pattern, severity), count in correlation_counts.items():
        rows.append(
            {
                "date": date_iso,
                "sis_version": sis_version_str,
                "type": "correlation",
                "kind_or_pattern": pattern,
                "severity": severity,
                "surface": "correlation",
                "confidence": None,
                "count": count,
                "files_scanned": total_files,
                "notes": None,
            }
        )
    rows.append(
        {
            "date": date_iso,
            "sis_version": sis_version_str,
            "type": "summary",
            "kind_or_pattern": "samples_processed",
            "severity": None,
            "surface": None,
            "confidence": None,
            "count": len(rows),
            "files_scanned": total_files,
            "notes": "Automated daily import",
        }
    )
    return rows


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Emit daily trend data for sis findings.")
    parser.add_argument(
        "--corpus",
        type=Path,
        required=True,
        help="Path to the corpus directory generated by mwb_corpus_pipeline.py",
    )
    parser.add_argument(
        "--glob",
        type=str,
        default="*.pdf",
        help="Glob pattern used by sis query (default %(default)s).",
    )
    parser.add_argument(
        "--out",
        type=Path,
        default=Path("daily.csv"),
        help="Path for the emitted CSV (appends if file exists).",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    version = sis_version()
    date_iso = datetime.now(timezone.utc).isoformat()

    pdf_files = find_pdf_files(args.corpus, args.glob)
    finding_counts = Counter()
    correlation_counts = Counter()
    valid_files = set()
    for pdf in pdf_files:
        if not is_pdf(pdf):
            print(f"skipping {pdf}: missing PDF header", file=sys.stderr)
            continue
        try:
            findings_records = run_query_file(pdf, "findings", "jsonl")
            partial_counts, _ = aggregate_findings(findings_records)
            finding_counts.update(partial_counts)
            correlation_records = run_query_file(pdf, "correlations", "jsonl")
            correlation_counts.update(aggregate_correlations(correlation_records))
            valid_files.add(str(pdf))
        except subprocess.CalledProcessError as err:
            print(f"sis query failed for {pdf}: {err}", file=sys.stderr)

    rows = build_rows(
        date_iso,
        version,
        len(valid_files),
        finding_counts,
        correlation_counts,
    )

    header = [
        "date",
        "sis_version",
        "type",
        "kind_or_pattern",
        "severity",
        "surface",
        "confidence",
        "count",
        "files_scanned",
        "notes",
    ]
    write_rows(args.out, rows, header)


if __name__ == "__main__":
    try:
        main()
    except subprocess.CalledProcessError as err:
        print("sis query failed:", err, file=sys.stderr)
        sys.exit(err.returncode)
