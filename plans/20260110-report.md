# Report output redesign recommendations

## Goals

- Make the report immediately actionable by summarising what was found, why it matters, and what to do next.
- Reduce cognitive load by grouping findings consistently by domain and analysis type.
- Preserve full detail without forcing readers to parse raw dumps.
- Support triage workflows with clear severity, confidence, and traceability back to artefacts.

## Proposed report structure (Markdown)

# Summary

Provide a concise, narrative summary that answers three questions:

- What was found?
- What is the impact?
- What next steps are recommended?

Suggested content:

- Overall verdict: `Benign`, `Suspicious`, or `Malicious` with confidence.
- High-level counts by severity and domain.
- Key drivers (top 3–5 findings by impact or risk).
- Recommended next steps with clear, testable actions.

Example summary format:

- Verdict: Suspicious (medium confidence)
- Findings: 2 high, 5 medium, 8 low across sizing, polyglot, and JavaScript domains.
- Impact: Document contains obfuscated JavaScript and a truncated XRef table consistent with evasive payloads.
- Recommended next steps:
  - Extract and review embedded JavaScript payloads (static and dynamic analysis).
  - Re-scan with `sis scan --deep` and compare object graph deltas.
  - If risk tolerance is low, treat as malicious and quarantine.

# Findings by scan domain

Organise the core of the report by domain first, then by category. This aligns with investigation workflows and keeps related signals together.

## Surface

### Sizing

- Summary of anomalies (e.g., inconsistent lengths, oversized streams).
- Detected evidence with references to object IDs and offsets.
- Likely impact and confidence.

### Polyglot

- File structure signals that indicate non-PDF content or multiple formats.
- Evidence such as header collisions, trailing data, or embedded formats.

### Other surface checks

- Header/version anomalies
- Trailer/XRef irregularities
- Incremental update chaining

## Structure

### Found contents

- Object counts, stream counts, xref usage, and outline of structural complexity.
- Any unusual object graph relationships.

### Content inventory

- JavaScript objects
- Embedded files
- Fonts
- Images and multimedia
- Forms and annotations

## Content details and analysis

### JavaScript analysis

#### Static analysis

- Summary of function signatures, deobfuscation indicators, suspicious APIs.
- Key findings with references to object IDs and scripts.

#### Dynamic analysis (sandboxed)

- Runtime behaviours, network intents, suspicious API usage.
- Execution traces and timing observations.

### Fonts

- Unusual embedded font programs or CFF tables.
- Known exploit patterns (if any).

### Embedded media

- Embedded files and their formats.
- Heuristic outcomes (e.g., payload-like content, archive nesting).

### Other content analysis

- Images, streams, or binary blobs that triggered heuristic detectors.

## Interactions

This section ties together signals across domains, which is critical for triage.

- Correlated findings (e.g., suspicious JavaScript linked to malformed stream length in the same object).
- Cross-domain chains (e.g., polyglot indicator plus embedded executable).
- Confidence adjustments based on correlation.

# ML analysis

## ML summary

- Model verdict and confidence.
- Top contributing features grouped by domain.

## ML details

- Feature breakdown and weights.
- Any thresholds or model version metadata.

# Chain analysis

Summarise observed chains that indicate a likely execution path or attack flow.

- Example: `OpenAction` → JavaScript → Embedded file extraction → Launch action.
- Provide evidence links for each chain step.

# Appendix

## List of findings and details

Provide a complete, structured list:

- Finding ID
- Severity and confidence
- Domain/category
- Evidence (object IDs, offsets, strings)
- Contextual notes

## Sis scanner details

- Scanner version, build, and configuration
- Scan profile (e.g., deep vs triage)
- Feature flags used
- Execution time, timestamp (with timezone)
- Environment metadata (platform, arch)

## Metadata

- File name, size, and hash values
- Any input provenance data provided by the caller

## Recommendations for implementation changes

1. Replace the current summary with a narrative summary that includes impact and next steps. Avoid listing only counts without context.
2. Group findings by domain and category, with short descriptive headings. Do not interleave findings from different domains.
3. Keep raw evidence in the appendix and link to it using consistent identifiers (e.g., `FND-001`).
4. Provide a dedicated interactions section to explain correlated findings and their implications.
5. Make ML output a separate section with a high-level summary and a detailed feature breakdown.
6. Ensure the report is human-readable in Markdown with consistent heading levels and short paragraphs.

## Notes

- Use Australian English spelling in all output.
- Keep the summary under 12 lines where possible.
- Ensure every recommendation has a clear action.
