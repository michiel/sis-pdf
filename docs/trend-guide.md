# Trend Monitoring Guide

Use this guide to turn the daily `sis` run on your corpus (generated by `scripts/mwb_corpus_pipeline.py`) into a Grafana-friendly dataset and dashboard. The workflow:

1. `scripts/trend_pipeline.py` collects findings + correlation summaries for every PDF in the corpus.
2. The script appends a `daily.csv` row per finding kind + correlation pattern; the file lives in a web-accessible directory.
3. Grafana Cloud ingests `daily.csv` and visualises trends (pattern count, severity, correlations, etc.).

## 1. Requirements

- `sis` binary on the PATH (the latest build is already installed on the VM).  
- Python 3.10+ (used by `scripts/trend_pipeline.py`).  
- Corpus directory created by `scripts/mwb_corpus_pipeline.py` (e.g., `/var/mwb/corpus`).  
- Webserver/HTTP access for `daily.csv` (e.g., `/var/www/html` or any directory served by your webserver).

## 2. Run the trend collection manually

```bash
python scripts/trend_pipeline.py \
  --corpus /var/mwb/corpus \
  --glob "*.pdf" \
  --out /var/www/html/daily.csv
```

- `--corpus`: points to the directory your corpus pipeline populates.  
- `--glob`: ensures only PDFs are scanned.  
- `--out`: writes/appends to `daily.csv`. The script preserves the header when the file is created and appends new rows when the script re-runs, so Grafana can pull a single rolling dataset.

The resulting CSV has columns like `date`, `sis_version`, `type` (finding/correlation/summary), `kind_or_pattern`, `severity`, `surface`, `confidence`, `count`, and `files_scanned`. Each run also records correlation summaries for `launch_obfuscated_executable`, `xfa_data_exfiltration_risk`, etc., plus totals for the current samples.

## 3. Integrate with your corpus cron job

Add the trend script immediately after `scripts/mwb_corpus_pipeline.py` in your existing cron job so the dataset includes the newest PDFs. Example crontab entry:

```
0 4 * * * /usr/bin/python3 ~/dev/sis-pdf/scripts/mwb_corpus_pipeline.py && \
    /usr/bin/python3 ~/dev/sis-pdf/scripts/trend_pipeline.py \
      --corpus /var/mwb/corpus \
      --glob "*.pdf" \
      --out /var/www/html/daily.csv \
      >> /var/log/trend_pipeline.log 2>&1
```

- Adjust paths to match your setup (`~` shorthand, corpus directory, log file).  
- `daily.csv` is overwritten only on first run; subsequent runs append new rows, so Grafana always reads the appended history.  
- The log file captures `sis query` failures (possible if `sis` cannot parse a PDF); resolve any errors before Grafana consumes the data.

## 4. Serve `daily.csv`

Place `daily.csv` under whatever document root your webserver exposes (e.g., `/var/www/html/daily.csv`). If you do not yet have a webserver, a lightweight option is:

```bash
cd /var/www/html
python3 -m http.server 8000
```

Keep that process running (systemd timer or screen) so Grafana can fetch `http://<vm-ip>:8000/daily.csv`. If you already use nginx/Apache, configure a static file on the same host pointing to the `daily.csv` path.

## 5. Grafana Cloud setup

Grafana Cloud (free tier) can visualise the CSV output without shipping data to another platform.

### a. Create account

1. Visit https://grafana.com/cloud and create a free-tier stack.  
2. After registration, note the Grafana endpoint and log in to the dashboard.

### b. Add the CSV data source

1. Navigate to **Configuration → Data sources**.  
2. Add the **CSV** plugin (available in the Grafana Cloud plugin catalog).  
3. Configure the data source:
   - **URL**: `http://<your-vm-ip-or-domain>/daily.csv`
   - **Refresh interval**: 5m (or your preferred cadence).  
   - **Column header row**: ensure the header row from `daily.csv` matches the column list (no spaces).  
4. Save & test the data source; it should list the 10 columns (`date`, `sis_version`, `type`, etc.).

### c. Build a dashboard

- **Panel 1 (Table)**: Query the CSV for `type=finding` and display `kind_or_pattern`, `severity`, `count`, `files_scanned`. Use the “Table” visualization.  
- **Panel 2 (Bar chart)**: Group by `kind_or_pattern` where `type=correlation`, sum `count` in the last 24 hours (Grafana CSV query language supports `WHERE`, `GROUP BY`).  
- **Panel 3 (Stat / Alert)**: Use `correlations.count` output (create a panel filtering for `type=correlation` and `count > 0`) to trigger alerting if any composite firing occurs.  
- Set panel refresh to match your cron (e.g., 5 mins). Grafana reads the CSV each time it renders, so new rows appear automatically.

### d. Optional: streaming ingestion

If you prefer JSONL ingestion instead of CSV, use the same script but point Grafana’s **SimpleJSON** or **JSON API** plugin at a small HTTP process that serves the latest JSONL produced by `sis query correlations --format jsonl`.

## 6. Monitoring regressions

To track regressions, store each row’s `sis_version`. When Grafana plots `count` versus `sis_version`, you immediately see when a regression (finding drop or correlation change) coincides with a new `sis` build. Use the `notes` column (populated for summary rows) to flag special events or anomalies when they are observed.

## 7. Verification

Run the script manually and view `daily.csv` to confirm values before letting cron run it:

```bash
python scripts/trend_pipeline.py --corpus /var/mwb/corpus --out /var/www/html/daily.csv
tail -n 20 /var/www/html/daily.csv
curl http://localhost:8000/daily.csv | head
```

If Grafana fails to read the file, double-check file permissions and ensure the HTTP server process runs under the same user or group that owns `daily.csv`.
